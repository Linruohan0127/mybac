
\chapter{人体姿态估计相关基础}
\echapter{Preface}


\section{深度学习基础}
\esection{}

机器学习(ML, Machine Learning)中深度学习(DL, Deep Learning)领域，是近几年蓬勃发展，在诸多场景下都大放异彩的一大研究方向，随着可用于学习的数据的增加，算力的增强以及视觉传感器的研发，深度学习在计算机视觉方面有着广泛的应用和优秀的表现。本节将针对深度学习发展到近年，构成深度学习基础的诸多经典模型及方法进行精炼的介绍。



\subsection{卷积神经网络}{}

卷积神经网络(CNN, Convolutional neural network) 是由LeCun于1989年提出的一种基于卷积运算神经网络，常用于在深度学习中对图像任务进行处理。

传统的神经网络由多层全连接层构成，对于图像信息，这种网络模型的训练参数十分庞大。而卷积神经网络通过稀疏连接，构建较小的感受野并共享参数，加以卷积计算，相比于传统神经网络大大降低了网络的复杂度，减少了计算量。并避免了传统神经网络反向传播过程中出现的梯度消失和梯度爆炸问题，允许了网络层数的加深。

同时，卷积神经网络最突出的特征就是能够尽可能地自动拟合所需要的特征。与其他图像分类算法相比，卷积神经网络对输入的数据格式要求极为简单，所需要的预处理较少，这意味着网络将更多地通过自动学习各种特征来优化卷积核。相对于传统的机器学习以及全连接网络等需要手工处理特征的模型，这种不依赖于先验知识和人工干预的特征提取是一个重要的优势，可以尽可能地保留原始信息，挖掘潜在规律。

此外，卷积神经网络对于具有网格状拓扑(如图像)的数据非常理想，因为在卷积和池化过程中，单独特征之间的空间关系也被加以提取。

在已有的工作中，可以看到这种特征提取可以拟合出人类对图像的直观理解。视觉神经系统突出的能力是同对物体的样例变化，几何变换，背景变化等线性与非线性的变化具有强大的识别能力。而卷积神经网络能够感知到的首先是色彩、亮度、对比度，再到边缘，角点等细节特征，更深层次可以提取出纹理，形状等复杂信息和特征，最后组织这些特征形成整个包含低、中、高维的认识。

卷积神经网络的隐藏层通常由两部分组成，第一部分为线性变换部分，包括卷积层、池化层和全连接层等，第二部分为非线性变换部分，如激活函数。本质上，卷积神经网络可以视为一个连续的线性加权滤波器接以非线性函数。

卷积神经网络的输入是一个张量，可以处理多维数据。卷积层由一组可学习的卷积核构成，每个卷积核对应一个范围较小，取决于卷积核大小的感受野。组成卷积核的每个元素都对应一个权重系数和一个偏移量，是进行特征提取的主要结构。卷积核会扫过输入的多维数据，并依次对每块感受野进行卷积操作，将结果输入至下一层。卷积操作即卷积核对感受野内的输入数据做矩阵元素乘法求和并叠加偏移量。

假设一个二维感受野的长与宽相等，如公式x中，b为偏差量，Z和Z表示第l层的卷积输入和输出，也被称为特征图（feature map），L为l的尺寸，Z对应特征图的像素，k为特征图的通道数，f、s和p是卷积层参数，对应卷积核大小、卷积步长（stride）和填充（padding）层数 。

经过卷积层后，图像被抽象为新的形状和维度的特征图，通常被送进池化层进行池化操作，起到非线性降采样的作用。在保证平移不变性、尺度不变性等情况下，对图像不同位置的特征聚合统计，对卷积层输出的特征映射进行筛选过滤，压缩参数量，精简计算过程。一般所用到的池化方法有最大值池化和平均池化。池化层不需要学习参数，仅需要指定池化类型，池化操作核的大小和池化操作的步长即可。最大值池化每次操作时指定窗口大小，保留每个窗口中最大的数值，分别作用于输入特征图的各个通道，通道之间 不会相互影响；平均池化是对邻域内特征点取平均。池化层的引入降低特征图的计算量，并具有一定的平移不变性，在整个过程中不会丢失重要信息，一定程度上可以降低网络模型训练时过拟合的概率，提升神经网络的泛化能力。在卷积神经网络架构中，通常会在连续的卷积层(每个卷积层后面通常跟着一个激活函数，比如ReLU层)之间周期性地插入一个池化层。

此外卷积神经网络在训练的过程中，随着层层的提取，数据的分布会不断地变化并加以积累。故而在通常在网络中加以批归一化层来抑制中间层数据分布发生变化的情况。同时有效避免了网络收敛速度慢，梯度爆炸等情况，加速模型训练。

通常在卷积神经网络的最后会加以全连接层实现最终的分类等任务。可以将整个模型理解为通过层层全连接和池化层将原视信息映射到了一个隐空间，并通常由全连接层将特征映射到样本标签空间。

经典的卷积神经网络模型有2012年Hinton等提出的AlexNet，以及2014年的VGG等。



\subsection{残差网络}{}

在越来越深的神经网络模型在图像相关竞赛中取得了可喜的胜利之后，深度残差网络可以说是近几年来计算机视觉及深度学习领域最具开创性的工作。残差结构使得网络可以训练到数百层且仍然具有较好的性能。

在图像相关的场景下，更深的网络意味着具有提取更多特征的可能性，故而堆叠越来越深的网络成为了一大趋势，用来丰富特征的维度。而在传统的深度学习网络模型的层数增加时，虽然在一些场景下网络表现出了更高的精度和准确性，但层数的增多也常常伴随着一些问题。例如梯(v)和梯度爆炸(exploding gradient)。

当采用基于梯度的学习方法和反向传播来训练神经网络的时候，每一个神经网络的权值在每次训练迭代中接收一个与误差函数相对于当前权值的偏导数成比例的更新，此时浅层的偏导即为多个很小的偏导的乘积。当网络加深时，浅层的网络梯度便接近于0。而当梯度很小时，网络的学习速度极低，在最坏的情况下，这可能会完全阻止神经网络进行进一步的训练。如果该层网络的梯度一直很小，无法得到充分的训练，该层对整个网络便几乎不做贡献。类似地，当使用导数可以取较大值的激活函数时，则更易遇到爆炸梯度问题。

事实上，实验表明，在进行传统的深度学习的训练过程中，随着ImageNet的网络层数的增加，最初越来越深的浅层网络的表现也越来越好，但当网络加深到一定阈值后，其准确率出现了退化(Degradation)，训练误差越来越大。

解决梯度消失问题最有效的方法之一便是He等于2016年提出的残差神经网络，构建了跳跃连接来创建信息的“高速公路”，允许梯度信息通过各个层。其中前一层的输出被添加到更深一层的输出，这使得来自网络早期部分的信息可以被传递到网络的更深层部分，从而帮助在更深的网络中维持信号传播，对深层的网络重新引入浅层的输出来补偿消失的数据，同时解决了网络加深时的退化问题。

图x为一个残差学习单元 (Residual Block)，由两部分组成，右侧曲线为跳层连接的直接映射部分(Shortcut Connection)，左侧曲线则包含卷积部分。此时，残差块的输出H(x)= F(x) + x，权重层实际上是在学习残差映射F(x)=H(x)-x。

随着ResNet在研究领域的日益普及，其体系结构也得到了越来越多的研究，构造了许多基于ResNet的新体系结构。如Xie等人提出的一种ResNet的变体ResNeXt，其构建模块如图x。

除此之外，Huang等人提出了一种名为DenseNet的新型架构。在ResNet中，跳跃连接主要是将距离较近的网络层之间进行连接，并让残差块串联组成整个网络来避免梯度消失和梯度爆炸以及网络的退化。DenseNet进一步利用了跳跃连接的效果，它使用跳跃连接，将所有神经网络层直接连接在一起。也就是说，对于每一层来说，其要处理的输入数据由全部的更浅层的特征图组成，其输出被传递到每个后续层。除了解决梯度消失的问题，这种架构还支持了特性重用，使网络具有很高的参数效率。



\section{欧拉角}
\esection{}
刚体运动(rigid motion)由旋转(rotation)和平移(translation)两部分组成，在描述人体的运动时，可以将人体抽象为由关节点组成的骨架，并将骨骼的运动近似为刚体运动。

对于已知的一次旋转变换，可以用旋转矩阵来表示。

而由于旋转矩阵有九个量，所以用它来描述只有三个自由度的一次旋转是冗余的。同时考虑到旋转矩阵具有行列式为1的正交矩阵的约束，使得后续针对旋转矩阵的估计和优化较为困难。故而本文选择更为直观和简洁的欧拉角来对骨骼的旋转运动进行最终的描述和记录。

欧拉角的基本原理是记录目标绕一个轴旋转的角度，并将一次旋转分解为三次绕不同的轴的旋转，并规定绕三个轴旋转的次序以及旋转轴在之前的旋转中的固定与否。最终，欧拉角描述旋转的形态为一个三维向量。

通常，在欧拉角的描述过程中，利用了两个坐标系，固定不动的世界坐标系xyz与随着物体的旋转而动的局部坐标系xyz。初始情况下，欧拉角描述的第一个旋转轴y与世界坐标系的y轴一致。根据上述旋转矩阵，可得欧拉角如下：

