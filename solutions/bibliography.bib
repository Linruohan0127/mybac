@inproceedings{p30,
  title={A Simple Yet Effective Baseline for 3d Human Pose Estimation},
  author={ Martinez, J.  and  Hossain, R.  and  Romero, J.  and  Little, J. J. },
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
}

@inproceedings{p29,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{p28,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={ Xie, S.  and  Girshick, R.  and P Doll√°r and  Tu, Z.  and  He, K. },
  booktitle={IEEE},
  year={2016},
 abstract={We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, codenamed ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart.},
}

@article{p27,
  title={Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
  author={ Ionescu, C.  and  Papava, D.  and  Olaru, V.  and  Sminchisescu, C. },
  journal={IEEE Transactions on Pattern Analysis, Machine Intelligence},
  volume={36},
  number={7},
  pages={1325-1339},
  year={2014},
 abstract={We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.},
}

@inproceedings{p26,
  title={3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training},
  author={ Pavllo, D.  and  FeichtenhofEr, C.  and  GrangiEr, D.  and  Auli, M. },
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
 abstract={In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D.},
}

@article{p25,
  title={Simple Baselines for Human Pose Estimation and Tracking},
  author={ Xiao, B.  and  Wu, H.  and  Wei, Y. },
  journal={arXiv e-prints},
  year={2018},
 abstract={There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.},
}

@article{p24,
  title={Cascaded Pyramid Network for Multi-Person Pose Estimation},
  author={ Chen, Y.  and  Wang, Z.  and  Peng, Y.  and  Zhang, Z.  and  Yu, G.  and  Sun, J. },
 abstract={The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, in- visible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these "hard" keypoints. More specifically, our algorithm includes two stages: Glob- alNet and RefineNet. GlobalNet is a feature pyramid net- work which can successfully localize the "simple" key- points like eyes and hands but may fail to precisely rec- ognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the "hard" keypoints by integrat- ing all levels of feature representations from the Global- Net together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation prob- lem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of- art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% rela- tive improvement compared with 60.5 from the COCO 2016 keypoint challenge.},
}

@inproceedings{p23,
  title={Stacked Hourglass Networks for Human Pose Estimation},
  author={ Newell, A.  and  Yang, K.  and  Jia, D. },
  booktitle={European Conference on Computer Vision},
  year={2016},
 abstract={This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
}

@article{p22,
  title={Deep Learning Based 2D Human Pose Estimation:A Survey},
  author={ Dang, Q.  and  Yin, J.  and  Wang, B.  and  Zheng, W. },
  journal={Tsinghua Science and Technology},
  year={2019},
 abstract={Human pose estimation has received significant attention recently due to its various applications in the real world. As the performance of the state-of-the-art human pose estimation methods can be improved by deep learning, this paper presents a comprehensive survey of deep learning based human pose estimation methods and analyzes the methodologies employed. We summarize and discuss recent works with a methodologybased taxonomy. Single-person and multi-person pipelines are first reviewed separately. Then, the deep learning techniques applied in these pipelines are compared and analyzed. The datasets and metrics used in this task are also discussed and compared. The aim of this survey is to make every step in the estimation pipelines interpretable and to provide readers a readily comprehensible explanation. Moreover, the unsolved problems and challenges for future research are discussed.},
}

@article{p21,
  title={Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods},
  author={Y Chen and Y Tian and  He, M. },
  journal={Computer Vision and Image Understanding},
  volume={192},
 abstract={Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.},
}

@article{p20,
  title={A survey on monocular 3D human pose estimation},
  author={ Ji, X.  and  Fang, Q.  and  Dong, J.  and  Shuai, Q.  and  Jiang, W.  and  Zhou, X. },
  journal={Virtual Reality & Intelligent Hardware},
  volume={2},
  number={ 6},
  pages={471-500},
  year={2020},
 abstract={Recovering human pose from RGB images and videos has drawn increasing attention in recent years owing to minimum sensor requirements and applicability in diverse fields such as human-computer interaction, robotics, video analytics, and augmented reality. Although a large amount of work has been devoted to this field, 3D human pose estimation based on monocular images or videos remains a very challenging task due to a variety of difficulties such as depth ambiguities, occlusion, background clutters, and lack of training data. In this survey, we summarize recent advances in monocular 3D human pose estimation. We provide a general taxonomy to cover existing approaches and analyze their capabilities and limitations. We also present a summary of extensively used datasets and metrics, and provide a quantitative comparison of some representative methods. Finally, we conclude with a discussion on realistic challenges and open problems for future research directions.},
}

@phdthesis{p19,
  title={Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊô∫ËÉΩ‰ΩìËßíËâ≤Âä®ÁîªËá™Âä®ÁîüÊàêÊäÄÊúØÁ†îÁ©∂},
  author={Âê¥Êñ∞Êúà},
  school={ÊµôÊ±üÂ§ßÂ≠¶},
 abstract={Êô∫ËÉΩ‰ΩìËßíËâ≤Âä®ÁîªËá™Âä®ÁîüÊàêÂú®Êú¨Ë¥®‰∏äÊòØÂØπÂÖ∑ÊúâËá™‰∏ªÊô∫ËÉΩË°å‰∏∫ÁöÑÁîüÂëΩ‰ΩìÁöÑ‰ªøÁúü,Âú®ÂΩ±ËßÜÁâπÊïàÂà∂‰Ωú„ÄÅÊ∏∏Êàè‰∏≠ÁöÑÈùûÁé©ÂÆ∂ËßíËâ≤ÊéßÂà∂„ÄÅËôöÊãüÁé∞ÂÆû„ÄÅ‰ªøÁúüÊµãËØïÁ≠âÈ¢ÜÂüüÈÉΩÊúâÈáçË¶ÅÂ∫îÁî®ÂâçÊôØ„ÄÇÂÆÉ‰∏ç‰ªÖÈúÄË¶ÅÂØπÊô∫ËÉΩ‰ΩìÁöÑÂÜÖÂú®Ë°å‰∏∫Ê®°ÂºèÂíåÂÜ≥Á≠ñËøõË°å‰ªøÁúüÂª∫Ê®°(Ë°å‰∏∫Âª∫Ê®°),‰πüÈúÄË¶ÅÂØπÊô∫ËÉΩ‰ΩìÁöÑÂ§ñÂú®Ë°å‰∏∫ËøõË°åÁúüÂÆûÂëàÁé∞(ËßíËâ≤Âä®Áîª),‰ªéËÄå‰ΩøÂæóÊô∫ËÉΩ‰ΩìËÉΩÂ§ü‰∏éÂ§ñÈÉ®ÁéØÂ¢ÉËøõË°åËá™‰∏ªÂíåÈÄºÁúüÁöÑ‰∫íÂä®„ÄÇÊú¨ÊñáÈíàÂØπÊó†‰∫∫È©æÈ©∂‰ªªÂä°‰∏≠ÁöÑÊô∫ËÉΩÁÆóÊ≥ï‰ªøÁúüÊµãËØïÈúÄÊ±Ç,ËÆæËÆ°ÂºÄÂèë‰∫ÜËá™‰∏ªÊô∫ËÉΩË°å‰∏∫ÁöÑÁîüÂëΩ‰Ωì‰ªøÁúüÂ∑•ÂÖ∑,Âπ∂ÈáçÁÇπÂØπÂèåË∂≥(‰∫∫)ÂíåÂõõË∂≥(Áâ≤Áïú)‰∏§ÁßçÊô∫ËÉΩ‰ΩìÁöÑËßíËâ≤Âä®ÁîªËá™Âä®ÊñπÊ≥ïËøõË°å‰∫ÜÁ†îÁ©∂,‰∏ªË¶ÅÂ∑•‰ΩúÂ¶Ç‰∏ã:1)Êô∫ËÉΩ‰ΩìÁöÑÂÜÖÈÉ®Ë°å‰∏∫Ê®°Âºè‰∏éÂÜ≥Á≠ñ‰ªøÁúü(Ë°å‰∏∫Âª∫Ê®°)„ÄÇÂºïÂÖ•Êô∫ËÉΩ‰ΩìÂØπËßíËâ≤ËøõË°åË°å‰∏∫Âª∫Ê®°,ÊääËßíËâ≤‰Ωú‰∏∫‰∏Ä‰∏™Áã¨Á´ãÁöÑË°å‰∏∫ËÄÖ,ËÉΩÂ§üÂØπ‰ªøÁúüÁéØÂ¢É‰ø°ÊÅØËøõË°åÊÑüÁü•,ËßÑÂàíË∑ØÂæÑ,ÁÑ∂ÂêéËøõË°åË°å‰∏∫ÊéßÂà∂ÂÜ≥Á≠ñ„ÄÇÈÄöËøáÂØπËßíËâ≤ËøõË°åÂü∫‰∫éÊô∫ËÉΩ‰ΩìÁöÑË°å‰∏∫Âª∫Ê®°,ËÉΩÊèêÈ´òËßíËâ≤Âä®ÁîªÁöÑËá™‰∏ªÊÄßÂíåÊô∫ËÉΩÊÄß„ÄÇ2)Êô∫ËÉΩ‰ΩìË°å‰∏∫ÁöÑÂ§ñÈÉ®ÂëàÁé∞(ËßíËâ≤Âä®Áîª)„ÄÇÂØπ‰∫éÂèåË∂≥ÂíåÂõõË∂≥Êô∫ËÉΩ‰ΩìËßíËâ≤ÂàÜÂà´ËÆæËÆ°ÂÆûÁé∞‰∫ÜÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑËßíËâ≤Âä®ÁîªËá™Âä®ÁîüÊàêÁöÑËß£ÂÜ≥ÊñπÊ°à,‰ΩøÂæó‰∏§ÁßçËßíËâ≤ËÉΩÂ§üÊ†πÊçÆÂú∞ÂäøÈ´ò‰ΩéÂÆûÊó∂ÁöÑÂÅöÂá∫ËøûË¥ØÁöÑ,ÂÖ∑ÊúâÁúüÂÆûÊÑüÂíåÂ§öÊ†∑ÊÄßÁöÑÂä®‰Ωú„ÄÇ3)Êô∫ËÉΩ‰ΩìËßíËâ≤Âä®ÁîªËá™Âä®ÁîüÊàêËß£ÂÜ≥ÊñπÊ°à‰∏éÂéüÂûãÁ≥ªÁªü„ÄÇÂØπ‰ª•‰∏äÂÖ≥ÈîÆÊäÄÊúØËøõË°åÊï¥ÂêàÂΩ¢ÊàêÊúÄÁªàÁöÑËß£ÂÜ≥ÊñπÊ°à,Âπ∂ÂàùÊ≠•ÂÆûÁé∞‰∫Ü‰∏Ä‰∏™Êô∫ËÉΩ‰ΩìËßíËâ≤Âä®ÁîªËá™Âä®ÁîüÊàêÂéüÂûãÁ≥ªÁªü„ÄÇÁ≥ªÁªüÈÄöËøáÂÆûÁé∞Â§ö‰∏™ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑËßíËâ≤Âä®ÁîªÂú∫ÊôØÂØπÊú¨ÊñáÊèêÂá∫ÁöÑËß£ÂÜ≥ÊñπÊ°àËøõË°å‰∫ÜÊµãËØïÂíåÈ™åËØÅ„ÄÇ},
}

@phdthesis{p18,
  title={Âü∫‰∫éÊ∑∑ÂêàÁâπÂæÅÁöÑÁßªÂä®Êú∫Âô®‰∫∫ÂêåÊ≠•ÂÆö‰Ωç‰∏éÂª∫ÂõæÁÆóÊ≥ïÁ†îÁ©∂},
  author={ÈÉëÊ≥ΩÁé≤},
  school={Âåó‰∫¨Â∑•‰∏öÂ§ßÂ≠¶},
 abstract={ÈöèÁùÄ‰∫∫Â∑•Êô∫ËÉΩ‰∫ß‰∏öÁöÑÂÖ¥Ëµ∑ÂØπÊú∫Âô®‰∫∫ÁöÑÊô∫ËÉΩÂåñÊèêÂá∫‰∫ÜÊõ¥È´òÁöÑË¶ÅÊ±Ç,ÂÖ∂‰∏≠Âü∫‰∫éËßÜËßâÁöÑÂêåÊ≠•ÂÆö‰ΩçÂíåÂª∫Âõæ(Simultaneous Localization and Mapping,SLAM)ÊäÄÊúØ,ÂÖ∑ÊúâËé∑Âèñ‰ø°ÊÅØ‰∏∞ÂØå,ÊàêÊú¨ËæÉ‰ΩéÁ≠âÁâπÁÇπÈÄêÊ∏êÂπøÊ≥õÂ∫îÁî®Âú®ÊúçÂä°Êú∫Âô®‰∫∫,Ëá™Âä®È©æÈ©∂,VR(Virtual Reality)Á≠âÈ¢ÜÂüü.ÁõÆÂâçËßÜËßâSLAM‰∏ªË¶ÅÊúâÁâπÂæÅÁÇπÊ≥ï,Áõ¥Êé•Ê≥ïÁ≠â,‰ΩÜÂùáÂ≠òÂú®Ëá™Ë∫´ÁöÑÂ±ÄÈôêÊÄß.ÁâπÂæÅÁÇπÊ≥ï‰∏ªË¶ÅÂà©Áî®ÂõæÂÉè‰∏≠ÁöÑÁÇπÁâπÂæÅÊù•ËøõË°å‰ΩçÂßø‰º∞ËÆ°,Âú®‰∏Ä‰∫õÁéØÂ¢ÉÂåÖÂê´ÁÇπÁâπÂæÅËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ãÂΩ±ÂìçÁÆóÊ≥ïÊïàÊûú.ÈíàÂØπ‰∏äËø∞ÈóÆÈ¢ò,Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ∑∑ÂêàÁâπÂæÅÊ®°ÂûãÁöÑSLAMÁÆóÊ≥ï.È¶ñÂÖà,Ëá™‰∏ªÊê≠Âª∫‰∫ÜÂÖ®ÂêëÁßªÂä®Âπ≥Âè∞,‰ª•‰æøÈááÈõÜÂõæÂÉè‰ø°ÊÅØÂπ∂ÊûÑÂª∫‰∫ÜÂü∫‰∫éÁéØÂ¢ÉËØçÂÖ∏ÁöÑÂä†ÈÄüÂåπÈÖçÊ®°Âûã;ÂÖ∂Ê¨°,ÈíàÂØπÁâπÂæÅÁÇπÊ≥ïÂú®ÁÇπÁâπÂæÅ‰∏çË∂≥ÁéØÂ¢É‰∏ãÊïàÊûúËæÉÂ∑ÆÁöÑÈóÆÈ¢ò,Êú¨ÊñáÊèêÂá∫‰∫ÜÊ∑∑ÂêàÁâπÂæÅÊ®°ÂûãËøõË°å‰ΩçÂßø‰º∞ËÆ°;ÊúÄÂêé,Êú¨ÊñáÈááÁî®Âü∫‰∫éÊúÄÂ§ßÂÖ±ËßÜÊùÉÈáçÂ∏ßÁöÑ‰ºòÂåñÁ≠ñÁï•ÂØπ‰ΩçÂßøËøõË°å‰ºòÂåñÂπ∂ÊúÄÁªàÂÆåÊàêÂú∞ÂõæÁöÑÊûÑÂª∫.ËÆ∫Êñá‰∏ªË¶ÅÁ†îÁ©∂ÂÜÖÂÆπÊúâÂ¶Ç‰∏ãÂá†‰∏™ÊñπÈù¢:(1)ÊûÑÂª∫‰∫åËøõÂà∂ÁéØÂ¢ÉËØçÂÖ∏ÂèäÂä†ÈÄüÂåπÈÖçÊ®°ÂûãÈíàÂØπÁâπÂæÅÁÇπSLAMÁÆóÊ≥ïÈÄöÂ∏∏Âä†ËΩΩÊñáÊú¨Á±ªÂûãËØçÂÖ∏ËÄóÊó∂ËæÉÈïø‰∏îÂú∫ÊôØÈíàÂØπÊÄßËæÉÂº±ÁöÑÈóÆÈ¢ò,Êú¨ÊñáÊûÑÂª∫‰∫Ü‰∫åËøõÂà∂ÁéØÂ¢ÉËØçÂÖ∏ÂèäÁâπÂæÅÂä†ÈÄüÂåπÈÖçÊ®°Âûã.Âà©Áî®Ê∑±Â∫¶Áõ∏Êú∫ËøõË°åÂõæÂÉèÊï∞ÊçÆÈááÈõÜÂπ∂ÂØπÊ∑±Â∫¶ÂõæËøõË°åÊª§Ê≥¢ÂéªÂô™,ÊèêÂèñÂΩ©Ëâ≤ÂõæÂÉè‰∏≠ÁöÑÁÇπÁâπÂæÅÂπ∂ÈááÁî®Bow(Bag of words)Â∫ìÂØπÂõæÂÉèÁÇπÁâπÂæÅËøõË°åËÅöÁ±ªÁîüÊàêËØçÂÖ∏,‰ΩøÁî®BOOSTÂ∫ìÂ∞ÜËØçÂÖ∏ËΩ¨‰∏∫‰∫åËøõÂà∂Êñá‰ª∂Âπ∂Ê†πÊçÆÁéØÂ¢ÉËØçÂÖ∏ÊûÑÂª∫Âä†ÈÄüÂåπÈÖçÊ®°Âûã.Âä†ÈÄüÂåπÈÖçÊ®°ÂûãÁº©Â∞è‰∫ÜÁâπÂæÅÂåπÈÖçËåÉÂõ¥‰∏îÂØπÊØîÊñáÊú¨Á±ªÂûãËØçÂÖ∏Êú¨ÊñáÊâÄÂª∫ËØçÂÖ∏ÂèñÂæóËæÉÂ•ΩÊïàÊûú.(2)Âü∫‰∫éÊ∑∑ÂêàÁâπÂæÅÊ®°ÂûãÁöÑÊú∫Âô®‰∫∫Áä∂ÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÈíàÂØπÁÇπÁâπÂæÅËøõË°å‰ΩçÂßø‰º∞ËÆ°Êó∂Âú®ÁâπÂæÅ‰∏çË∂≥ÊàñÁº∫Â§±ÁöÑÊÉÖÂÜµ‰∏ã‰ΩçÂßø‰º∞ËÆ°ÊïàÊûúËæÉÂ∑ÆÁöÑÈóÆÈ¢ò,Êú¨ÊñáÊèêÂá∫‰∫ÜÂü∫‰∫éÊ∑∑ÂêàÁâπÂæÅÁöÑ‰ΩçÂßø‰º∞ÁÆóÊñπÊ≥ï.ÊèêÂèñÂõæÂÉè‰∏≠ÁöÑÁÇπÁ∫øÁâπÂæÅÂπ∂Âà©Áî®ÊîπËøõÈöèÊú∫ÈááÊ†∑‰∏ÄËá¥ÁÆóÊ≥ï(Random sample consensus,RANSAC)ÂâîÈô§ÈîôËØØÁöÑÂåπÈÖçÁâπÂæÅ.ÁÑ∂ÂêéÊ†πÊçÆÁâπÂæÅÁÇπÈááÁî®EPnP(Efficient Perspective-n-Point)ÊñπÊ≥ï‰º∞ÁÆóÊú∫Âô®‰∫∫‰ΩçÂßø.Ê≠§Â§ñÊú¨ÊñáÊ†πÊçÆÁÅ≠ÁÇπÂéüÁêÜÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂçïÂπÖÂõæÂÉè‰∏≠Á∫øÊÆµÁâπÂæÅ‰º∞ÁÆó‰ΩçÂßøÊñπÊ≥ï,Â∞Ü‰∏§Áßç‰º∞ÁÆóÁªìÊûúËûçÂêàÂêéÊèêÈ´ò‰∫ÜÁÆóÊ≥ïÈ≤ÅÊ£íÊÄß.(3)Âü∫‰∫éÊúÄÂ§ßÂÖ±ËßÜÊùÉÈáçÂ∏ßÁöÑÂÖ®Â±Ä‰ºòÂåñÂíåÂú∞ÂõæÂàõÂª∫ÈíàÂØπÂêéÁ´Ø‰ºòÂåñËÆ°ÁÆóÈáèÂ§ß‰ª•ÂèäÁÇπ‰∫ëÂú∞ÂõæÊâÄÂç†ÂÜÖÂ≠òËæÉÂ§öÁöÑÈóÆÈ¢ò,Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊúÄÂ§ßÂÖ±ËßÜÊùÉÈáçÂ∏ßÁöÑÂÖ®Â±Ä‰ºòÂåñÊñπÊ≥ï.Âú®ÂæóÂà∞ÂÖ≥ÈîÆÂ∏ßÂêé,ÁªüËÆ°ÂÖ≥ÈîÆÂ∏ß‰πãÈó¥ÁöÑÂÖ±ËßÜÂú∞ÂõæÁÇπÁöÑÊï∞Èáè‰Ωú‰∏∫ÂÖ±ËßÜÊùÉÈáç,ÈÄöËøáËÆæÂÆöÂÖ±ËßÜÊùÉÈáçÈòàÂÄºÊù•ÊéßÂà∂ÂèÇ‰∏é‰ºòÂåñÁöÑÂÖ≥ÈîÆÂ∏ß.ÂæóÂà∞‰ºòÂåñÁªìÊûúÂêéÂ∞ÜÁÇπ‰∫ëÂõæËΩ¨‰∏∫ÂÖ´ÂèâÊ†ëÂú∞ÂõæÂπ∂ÊúÄÁªàÂÆåÊàêÂàõÂª∫ÂÖ®Â±Ä‰∏ÄËá¥ÁöÑÂú∞Âõæ.Â∞ÜÊú¨ÊñáÊâÄÊèêÁÆóÊ≥ï‰∏éORBSALMÁÆóÊ≥ïËøõË°åÂØπÊØî,‰ΩøÁî®TUMÂÖ¨ÂºÄÁöÑRGBDÊï∞ÊçÆÈõÜ‰∏≠fr1/roomÂíåfr2/deskÂØπ‰∫åËÄÖËøõË°åÊµãËØï.Áõ∏ÊØî‰∫éORBSLAMÁÆóÊ≥ï,ÈááÁî®Êú¨ÊñáÊâÄÊèêÁöÑÂü∫‰∫éÊ∑∑ÂêàÁâπÂæÅÁöÑÁßªÂä®Êú∫Âô®‰∫∫ÂêåÊ≠•ÂÆö‰ΩçÂíåÂú∞ÂõæÂàõÂª∫ÊñπÊ≥ïÂùáÊñπÊ†πËØØÂ∑ÆÂàÜÂà´‰∏ãÈôç2.27cm,4.87cm,È™åËØÅ‰∫ÜÁÆóÊ≥ïÁöÑÊúâÊïàÊÄß.Âà©Áî®Ëá™‰∏ªÊê≠Âª∫ÁöÑÂÖ®ÂêëÁßªÂä®Âπ≥Âè∞Âú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ËøõË°åÊú∫Âô®‰∫∫ÂêåÊ≠•ÂÆö‰ΩçÂíåÂú∞ÂõæÊûÑÂª∫ÊúÄÁªàÂæóÂà∞‰∫ÜÂÖ®Â±Ä‰∏ÄËá¥ÁöÑÂú∞Âõæ.},
}

@phdthesis{p17,
  title={Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Á†îÁ©∂},
  author={Ë∞¢Â≠êÂ®Å},
  school={Âåó‰∫¨ÈÇÆÁîµÂ§ßÂ≠¶},
  year={2019},
 abstract={3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊòØÁõÆÂâçÊúÄÁÉ≠Èó®ÁöÑÁ†îÁ©∂È¢ÜÂüü‰πã‰∏Ä,ÂÖ∂‰∏ªË¶ÅÁõÆÁöÑÊòØ‰ªéÂõæÂÉèÊàñËßÜÈ¢ë‰∏≠ÂØπ‰∫∫‰ΩìÂÖ≥ËäÇÁÇπ‰ΩçÁΩÆËøõË°å‰º∞ËÆ°,ËøõËÄåÁªÑÊàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ‰∫∫‰ΩìÂßøÊÄÅ.3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÂèØ‰ª•‰Ωú‰∏∫‰∫∫‰ΩìÂßøÊÄÅËØÜÂà´,Ë°å‰∏∫ËØÜÂà´,‰∫∫‰ΩìË∑üË∏™Á≠â‰ªªÂä°ÁöÑÂü∫Á°Ä,ÂêåÊó∂‰πüÂú®Â∫∑Â§çÂåªÁñó,ËßÜÈ¢ëÁõëÊéß,È´òÁ∫ß‰∫∫Êú∫‰∫§‰∫íÁ≠âÈ¢ÜÂüüÂÖ∑ÊúâÂæàÈ´òÁöÑÂ∫îÁî®‰ª∑ÂÄº.ËøëÂá†Âπ¥Êù•,ÈöèÁùÄ‰ª•Á•ûÁªèÁΩëÁªú‰∏∫Ê†∏ÂøÉÁöÑÊ∑±Â∫¶Â≠¶‰π†Âú®Â§ö‰∏™È¢ÜÂüüÁöÑÊàêÂäüÂ∫îÁî®,ÂÖ∑ÊúâÂº∫Â§ßÁöÑÂ≠¶‰π†ËÉΩÂäõÁöÑÊ∑±Â∫¶Á•ûÁªèÁΩëÁªú‰πüÈÄêÊ∏êÊàê‰∏∫‰∫ÜÂ¶Ç3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Âú®ÂÜÖÁöÑÂ§çÊùÇ‰ªªÂä°ÁöÑÊúÄ‰Ω≥ÈÄâÊã©,ËÄåÂú®ÊúÄÊñ∞ÁöÑÁ†îÁ©∂‰∏≠‰πüÂÜçÊ¨°È™åËØÅ‰∫ÜÂÖ∂‰ºòÂºÇÊÄß.ÊâÄ‰ª•Êú¨ÊñáÈíàÂØπ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°‰∏≠ÈÅÆÊå°Âπ≤Êâ∞Á≠âÈóÆÈ¢ò,Âú®Ê∑±Â∫¶Â≠¶‰π†ÁöÑÊ°ÜÊû∂‰∏ã,ÊèêÂá∫‰∫ÜÂØπÁΩëÁªúÁªìÊûÑÂèäÂá†‰ΩïÁ∫¶ÊùüÁöÑÊîπËøõ,ÂÖ∑‰ΩìÂÜÖÂÆπÂ¶Ç‰∏ã:1.ÁõÆÂâç3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ï‰∏≠Âú®ÂØπ3D‰∫∫‰ΩìÂÖ≥ËäÇÁÇπËøõË°åÂõûÂΩíÊó∂ÈÉΩÊ≤°ÊúâÂÖÖÂàÜËÄÉËôëÂà∞ÂõæÂÉèÁöÑÁ©∫Èó¥ÁâπÊÄß.ÊâÄ‰ª•Êú¨Êñá‰ª•ÊèêÂçá3D‰∫∫‰ΩìÂÖ≥ËäÇÁÇπÂõûÂΩíÂØπÂõæÂÉèÁ©∫Èó¥ÁâπÊÄßÁöÑÊçïÊçâËÉΩÂäõ‰Ωú‰∏∫ÂàáÂÖ•ÁÇπ,ÊèêÂá∫‰∏Ä‰∏™Âü∫‰∫éÂ§öÂ∞∫Â∫¶ÈáçÊ†°È™åÊñπÊ≥ïÁöÑ3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁΩëÁªúÊ®°Âûã,ÈÄöËøáËøÅÁßªÂ≠¶‰π†ÁöÑÊñπÂºèÂêëÂéüÁΩëÁªú‰∏≠ÂºïÂÖ•Áî®‰∫éÊçïÊçâÂ§ö‰∏™Â∞∫Â∫¶‰∏ãÂõæÂÉèÁ©∫Èó¥ÁâπÊÄßÁöÑÈáçÊ†°È™åÊóÅË∑Ø,ËøõËÄåÊèêÂçá3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß.Êú¨Êñá‰ªéÂÆöÈáèÂíåÂÆöÊÄßÁöÑËßíÂ∫¶ÂØπÁΩëÁªúÊ®°ÂûãÊúâÊïàÊÄßËøõË°å‰∫ÜÈ™åËØÅ,Âπ∂Âú®Human3.6m‰∫∫‰ΩìÂßøÊÄÅÊ†áÊ≥®Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÂÖ∑ÊúâÁ´û‰∫âÂäõÁöÑÁªìÊûú.2.ÈíàÂØπ‰∫∫‰ΩìÂßøÊÄÅËøõË°å‰º∞ËÆ°Êó∂Âá∫Áé∞‰∏Ä‰∫õÁâπÊÆäÁöÑÂßøÊÄÅ,‰ªÖËÄÉËôëÂØπÁΩëÁªúËøõË°å‰ºòÂåñÂØπ‰∫éËØ•Á±ªÁâπÊÆäÂßøÊÄÅÁöÑ‰º∞ËÆ°ÊïàÊûúÊèêÂçáÊòØÊúâÈôêÁöÑ,ÈúÄË¶ÅÂºïÂÖ•‰∫∫‰ΩìÂá†‰ΩïÁ∫¶Êùü.Áî±Ê≠§Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êõ¥Âº∫ÁöÑ‰∫∫‰ΩìÂßøÊÄÅÂá†‰ΩïÁ∫¶ÊùüÊ®°Âûã,Âπ∂Â∞ÜÂÖ∂ËøêÁî®‰∫é3D‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂçäÁõëÁù£Â≠¶‰π†‰∏≠,ËØ•Ê®°Âûã‰ªéÂÖ≥ËäÇÁÇπ‰ΩçÁΩÆ,È™®È™ºÈïøÂ∫¶ÂèäÂÖ≥ËäÇËßíÂ∫¶‰∏â‰∏™ÊñπÈù¢ÂØπ‰∫∫‰ΩìÂßøÊÄÅËøõË°åÁ∫¶Êùü,ÂêåÊó∂Â∞ÜÂÖ∂Â∫îÁî®‰∫éÊçüÂ§±Â±Ç,Âπ∂ËøõË°å‰∫ÜÊúâÊïàÊÄßÂàÜÊûê,ÊúÄÂêéÂú®ÂÆöÈáèÂíåÂÆöÊÄßËßíÂ∫¶ËøõË°å‰∫ÜÂØπÊØîÂÆûÈ™å,Âπ∂ÂØπÁªìÊûúËøõË°å‰∫ÜÂàÜÊûê‰∏éÈ™åËØÅ.},
}

@phdthesis{p16,
  title={Âü∫‰∫é3DÈ™®Êû∂ÁöÑËÇ¢‰ΩìÂä®‰ΩúËØÜÂà´Á†îÁ©∂},
  author={ÈôàÊòüÂÆè},
  school={ÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶},
  year={2019},
 abstract={ËÇ¢‰ΩìÂä®‰ΩúËØÜÂà´Âõ†‰∏∫ÂÖ∂Âú®Êô∫ËÉΩËßÜÈ¢ëÁõëÊéß,‰∫∫Êú∫‰∫§‰∫í,ËæÖÂä©ÊäÄÊúØÁ≠âÈ¢ÜÂüüÁöÑÈáçË¶Å‰ΩúÁî®,Ë∂äÊù•Ë∂äÂèóÂà∞‰∫∫‰ª¨ÁöÑÂπøÊ≥õÂÖ≥Ê≥®.ÁÑ∂ËÄåÁî±‰∫é‰∫∫‰ΩìÂßøÊÄÅÊú¨Ë∫´ÁöÑÂ§öÊ†∑ÊÄß,‰ª•ÂèäÂä®‰ΩúËØ≠‰πâÁöÑÊ®°Á≥äÊÄßÂíåËÇ¢‰ΩìËøêÂä®ÁöÑÂ§öÂèòÊÄß,‰ΩøÂæóÂä®‰ΩúËØÜÂà´Âú®Â§ö‰∏™ÊñπÈù¢ÈÉΩÈù¢‰∏¥ÁùÄÈáçÂ§ßÁöÑÊåëÊàò.ÁõÆÂâç,Èì∞Êé•ÂºèÁöÑ‰∫∫‰ΩìÂßøÊÄÅ(‰πüË¢´Áß∞‰∏∫È™®Êû∂)ËÉΩ‰∏∫ÊèèËø∞‰∫∫‰ΩìÂä®‰ΩúÊèê‰æõÈùûÂ∏∏Â•ΩÁöÑË°®ÂæÅ,ÈÇ£‰πàÂ¶Ç‰ΩïËé∑Âèñ‰∫∫‰ΩìÂÖ≥ËäÇÁÇπ,Âπ∂‰∏îÂàÜÊûêÂÖ≥ËäÇÁÇπÂú®ËøêÂä®Êó∂ÁöÑÁõ∏ÂÖ≥ÊÄß,‰ª•Ê≠§Êù•ËææÂà∞Âä®‰ΩúËØÜÂà´ÁöÑÁõÆÁöÑ‰æøÊàê‰∏∫‰∫ÜÈöæÁÇπÈóÆÈ¢ò.Êú¨Êñá‰∏ªË¶ÅÈíàÂØπ‰ª•‰∏äÈóÆÈ¢ò,ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈùûÂèÇÊï∞ÁºñÁ†ÅÁ∫¶ÊùüÁöÑÂåπÈÖçÊñπÊ≥ïÊèêÂèñ‰∫∫‰Ωì3DÈ™®Êû∂,Âπ∂Âà©Áî®ÊâÄÂæóÈ™®Êû∂Â∫èÂàóÂª∫Á´ã‰∫∫‰ΩìÈ™®Êû∂Êó∂Á©∫Âõæ,ÈÄöËøáÂõæÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊù•ÂÆåÊàê‰∫∫‰ΩìÂä®‰ΩúÂàÜÁ±ª‰ªªÂä°,ÊúÄÁªàÂ∞Ü‰ª•‰∏äÂêÑÈÉ®ÂàÜÂ∫îÁî®‰∫éVR‰∫§‰∫íËøáÁ®ã,ÂÆûÁé∞‰∫∫‰ΩìÂä®‰ΩúËØÜÂà´Âú®ËØ•È¢ÜÂüüÁöÑÂ∫îÁî®.ËÆ∫Êñá‰∏≠‰∏ªË¶ÅÂ∑•‰ΩúÂÜÖÂÆπ‰∏éÂàõÊñ∞ÁÇπÂ¶Ç‰∏ã:(1)ÈíàÂØπ‰ªéÂçïÁõÆÊëÑÂÉèÊú∫ÊâÄÈááÈõÜÂõæÂÉè‰∏≠ÊèêÂèñ‰∫∫‰Ωì3DÈ™®Êû∂ÁöÑÈóÆÈ¢ò,Êàë‰ª¨Â∞ÜÂÖ∂ÂàÜ‰∏∫‰∏â‰∏™ÈÉ®ÂàÜËøõË°å,ÂàÜÂà´ÊòØÈ¶ñÂÖà‰ΩøÁî®OpenposeÂºÄÊ∫êÂ∫ìÊèêÂèñ‰∫∫‰Ωì2DÈ™®Êû∂‰ø°ÊÅØ,ÂÖ∂Ê¨°ÈÄöËøáÂàÜÊûê2D‰∏é3DÈ™®Êû∂ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª,Âü∫‰∫éÈùûÂèÇÁºñÁ†ÅÁ∫¶ÊùüÂª∫Á´ã‰∫∫‰ΩìÂßøÊÄÅÂ∫ì,ÊúÄÁªà‰ª•Ê¨ßÊ∞èË∑ùÁ¶ª‰∏∫Ê†áÂáÜ,Âà©Áî®ÂåπÈÖçÁöÑÊñπÊ≥ïÂú®ÂßøÊÄÅÂ∫ì‰∏≠ÊâæÂà∞2DÈ™®Êû∂ÂÖ∂ÂØπÂ∫îÁöÑ3DÈ™®Êû∂.Êï¥‰∏™ÁÆóÊ≥ï‰∏ç‰ªÖÊèêÈ´ò‰∫Ü3DÈ™®Êû∂ÊèêÂèñÁöÑÂáÜÁ°ÆÊÄß,Âπ∂‰∏î‰øùËØÅ‰∫ÜÈ™®Êû∂ÊèêÂèñËøáÁ®ãÁöÑÂÆûÊó∂ÊÄß.(2)ÂØπ‰∫é‰∫∫‰ΩìÂä®‰ΩúÂàÜÁ±ªÈóÆÈ¢ò,Êàë‰ª¨‰ΩøÁî®‰∫∫‰Ωì3DÈ™®Êû∂Â∫èÂàó,ÊûÑÂª∫‰∫∫‰ΩìÈ™®Êû∂Êó∂Á©∫Âõæ,‰ΩøÂÖ∂ÂÖÖÂàÜÂåÖÂê´‰∫∫‰ΩìÂä®‰ΩúÁöÑÊó∂Á©∫‰ø°ÊÅØ,ÁÑ∂ÂêéÂà©Áî®ÂõæÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊù•ÊèêÂèñÊó∂Á©∫Âõæ‰∏≠ÁöÑÁâπÂæÅ,‰ªéËÄåÂ∞ÜÂéüÈóÆÈ¢òËΩ¨Êç¢‰∏∫ÂõæÁöÑÁâπÂæÅÊèêÂèñÈóÆÈ¢ò.Âú®Ê≠§ËøáÁ®ã‰∏≠,Êàë‰ª¨ÊûÑÂª∫‰∫ÜÂØπ‰∫é‰∫∫‰ΩìÈ™®Êû∂Êó∂Á©∫ÂõæÁöÑÂõæÂç∑ÁßØÊ†∏,Âπ∂‰∏∫Ê†πËäÇÁÇπÁöÑÊÑüÂèóÈáéÂ≠êÈõÜÂàíÂàÜÈóÆÈ¢òÊèê‰æõ‰∏çÂêåÁöÑÂàíÂàÜÁ≠ñÁï•.ÊúÄÁªàÊê≠Âª∫ÂõæÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊ®°Âûã,ÈÄöËøáÂÆûÈ™åËØÅÊòéËØ•Ê®°ÂûãÂú®Ëá™Â∑±Â§ÑÁêÜÁöÑÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÂæàÂ•ΩÁöÑÊïàÊûú.(3)‰ª•973Ê∑±Á©∫Êé¢ÊµãÂô®ÁªÑÂêàËá™‰∏ªÂØºËà™È°πÁõÆ‰∏∫‰æùÊâò,Âπ∂Â∞ÜÂÖ∂‰∏≠ÁöÑËôöÊãüÁé∞ÂÆûÊ®°Âùó‰Ωú‰∏∫Â∫îÁî®Âú∫ÊôØ,Êê≠Âª∫Ê∑±Á©∫Êé¢ÊµãÂô®VRÊòæÁ§∫‰ªøÁúüÂπ≥Âè∞.ÂàõÊñ∞ÊÄßÁöÑÂ∞ÜËÇ¢‰ΩìÂä®‰ΩúËØÜÂà´Â∫îÁî®‰∫éVR‰∫§‰∫íÈ¢ÜÂüü,Âà©Áî®ÊâÄÁ†îÁ©∂ÁöÑ‰∫∫‰Ωì3DÈ™®Êû∂ÊèêÂèñ,ÂõæÂç∑ÁßØ‰∫∫‰ΩìÂä®‰ΩúÂàÜÁ±ªÊäÄÊúØ,‰∏≤ËÅîÊàêËæÉ‰∏∫ÂÆåÊï¥ÁöÑÈó≠ÁéØ‰∫§‰∫íÊ®°Âºè,Âπ∂‰∏îÊé¢ËÆ®‰∫Ü‰∫∫‰ΩìÂä®‰ΩúËØ≠‰πâ‰∏é‰∏ÄËà¨‰∫∫Êú∫‰∫§‰∫íËøáÁ®ãÁöÑÂÖ≥Á≥ª.ÊúÄÁªàÂà©Áî®ÂÆûÈ™åËØÅÂÆûÂÖ∂ÂèØË°åÊÄß,‰∏éÂêÑ‰∏™‰∫§‰∫íÁéØËäÇÁöÑÂèØÈù†ÊÄß.},
}

@phdthesis{p15,
  title={Âü∫‰∫éË∑®Èò∂ÊÆµÊ∑±Â∫¶ÁΩëÁªúÁöÑ‰∫∫‰ΩìÂßøÊÄÅÂàÜÊûê},
  author={Âë®‰∫öËæâ},
  school={ÂêàËÇ•Â∑•‰∏öÂ§ßÂ≠¶},
 abstract={Âú®ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠,Âü∫‰∫éÂõæÂÉèÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊòØ‰∏ÄÈ°πÈáçË¶ÅËÄåÂèàÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°„ÄÇÂÆÉÊòØÊåáÂú®ÁªôÂÆöÂõæÂÉè‰∏≠ÂØπÁõÆÊ†á‰∫∫Áâ©ÁöÑÂÖ≥ËäÇÁÇπ‰ΩçÁΩÆËøõË°åÂÆö‰Ωç,Âü∫‰∫éÂõæÂÉèÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊòØ‰∏Ä‰∫õËÆ°ÁÆóÊú∫ËßÜËßâÈ´òÁ∫ß‰ªªÂä°ÁöÑÂü∫Á°Ä,Â¶Ç‰∫∫Á±ªË°å‰∏∫ËØÜÂà´„ÄÅÂõæÂÉèÊ£ÄÁ¥¢Á≠â„ÄÇÁÑ∂ËÄåÁî±‰∫é‰∫∫‰ΩìËÇ¢‰ΩìÁöÑÂÖ≥ËäÇÂ§çÊùÇÂ∫¶„ÄÅÈÅÆÊå°‰ª•ÂèäÊ∑∑ÊùÇËÉåÊôØÁ≠âÊÉÖÂÜµÁöÑÂΩ±Âìç,ÂáÜÁ°ÆÂÆö‰ΩçÂÖ≥ËäÇÁÇπ‰ΩçÁΩÆ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇÂú®Áé∞ÊúâÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÁöÑÁ†îÁ©∂Âü∫Á°Ä‰∏ä,ÂºïÂÖ•‰∏ÄÁßçË∑®Èò∂ÊÆµÁöÑÁªìÊûÑÊù•ÂÆö‰ΩçÂõæÂÉè‰∏≠ÁõÆÊ†á‰∫∫Áâ©ÁöÑÂÖ≥ËäÇÁÇπ,ÊèêÂçá‰∫ÜÂÆö‰ΩçÁöÑÂáÜÁ°ÆÂ∫¶,Âπ∂ËÆæËÆ°ÊúâÊïàÁöÑÂü∫‰∫éÂèØ‰ø°ÂÖ≥ËäÇÁÇπÁΩÆ‰ø°Â∫¶Êé®ÁêÜÁöÑÊñπÊ≥ïÊù•ÂØπÊ£ÄÊµãÁªìÊûúËæÉÂ∑ÆÁöÑ‰∫∫‰ΩìÂÖ≥ËäÇÁÇπËøõË°åÊîπËøõÁöÑÂêéÂ§ÑÁêÜÊäÄÊúØ„ÄÇÊú¨ÊñáÁöÑ‰∏ªË¶ÅÂ∑•‰ΩúÂ¶Ç‰∏ã:(1)ÈòêËø∞‰∫Ü‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÂü∫Êú¨ÊµÅÁ®ã,ÊåâÁÖßÈùûÊ∑±Â∫¶ÂíåÊ∑±Â∫¶Ê®°Âûã‰ªãÁªç‰∫ÜÂá†‰∏™ÁªèÂÖ∏ÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Ê®°ÂûãÂπ∂ÂàÜÊûê‰∫ÜÂÆÉ‰ª¨ÂêÑËá™ÁöÑ‰ºòÁº∫ÁÇπ,‰∏∫ËÆ∫ÊñáÂ∑•‰ΩúÁöÑÂºÄÂ±ïÂ•†ÂÆö‰∫ÜÁêÜËÆ∫Âü∫Á°Ä„ÄÇ(2)ËÆæËÆ°‰∫Ü‰∏ÄÁßçË∑®Èò∂ÊÆµÁªìÊûÑÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Ê®°Âûã,ËØ•Ê®°ÂûãÈÄöËøáÁ¨¨‰∏ÄÈò∂ÊÆµÁΩëÁªúÊèêÂèñÂàùÂßãÁâπÂæÅ,Âπ∂ÈááÁî®Â§öÈò∂ÊÆµÁΩëÁªúÊ®°ÂûãÊèèËø∞‰∏çÂêåÂ∞∫Â∫¶‰∏ãÁâπÂæÅÂØπÂÖ≥ËäÇÁÇπÂÆö‰ΩçÁöÑ‰ΩúÁî®„ÄÇÂêåÊó∂,Âà©Áî®Ë∑®Èò∂ÊÆµÊñπÂºèÂ∞ÜËØ•ÂàùÂßãÁâπÂæÅ‰∏éÂêÑÂ∞∫Â∫¶ÁâπÂæÅ‰∏≤ËÅî,ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂ§öÂ∞∫Â∫¶ÂÖ≥ËäÇÁÇπÂÆö‰ΩçÁöÑËÅîÂêàÊçüÂ§±,Áº©Áü≠‰∫ÜËæìÂá∫Á´ØÂÆö‰ΩçËØØÂ∑ÆÂà∞ÂêÑÂ∞∫Â∫¶ÂÆö‰ΩçËØØÂ∑ÆÁöÑËÆ°ÁÆóË∑ØÂæÑ,ÂÆûÁé∞ÊúâÊïàÁöÑ‰∫∫‰ΩìÂÖ≥ËäÇÁÇπÂàùÂßãÁâπÂæÅÂ≠¶‰π†„ÄÇ(3)ÈíàÂØπ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Ê®°ÂûãÈöæ‰ª•ÂØπÈÅÆÊå°ÂßøÊÄÅËøõË°åÊúâÊïàÂ§ÑÁêÜÁöÑÈóÆÈ¢ò,Êú¨ÊñáËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂèØ‰ø°ÂÖ≥ËäÇÁÇπÁöÑÁΩÆ‰ø°Â∫¶Êé®ÁêÜËøáÁ®ãÁöÑÊ®°Âûã„ÄÇÂ∞ÜÂßøÊÄÅ‰º∞ËÆ°Ê®°ÂûãËæìÂá∫ÁöÑÂêÑÂÖ≥ËäÇÁÇπÁΩÆ‰ø°Â∫¶ÂõæËøõË°åÂÖ≥ËäÇÁÇπÂèØËßÅÊÄßÂà§Êñ≠,Â∞ÜÂÖ∂ÂàÜ‰∏∫‰∏§ÁªÑÂèØ‰ø°ÁöÑ‰∏é‰∏çÂèØ‰ø°ÁöÑÂÖ≥ËäÇÁÇπ,ÁïôÂèñÂèØ‰ø°ÁöÑÂÖ≥ËäÇÁÇπÁΩÆ‰ø°Â∫¶Âõæ,Âπ∂Ê†πÊçÆ‰∫∫‰Ωì‰ªªÊÑè‰∏§‰∏™ÂÖ≥ËäÇÁÇπ‰πãÈó¥ÁöÑÈ´òÊñØÊù°‰ª∂ÂàÜÂ∏É,ÂØπÈÅÆÊå°ÂØºËá¥ÁöÑ‰∏çÂèØ‰ø°ÂÖ≥ËäÇÁÇπËøõË°åÁΩÆ‰ø°Â∫¶Êé®ÁêÜ‰ª•ÂÆûÁé∞ÂØπÈÅÆÊå°ÂßøÊÄÅÁöÑÊúâÊïàÂ§ÑÁêÜ,ÊèêÂçá‰∫ÜÊú¨ÊñáÊ®°ÂûãÂú®ÈÅÆÊå°ÊÉÖÂÜµ‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ},
}

@phdthesis{p14,
  title={Âü∫‰∫éKinectÂä®‰ΩúÈ©±Âä®ÁöÑ‰∏âÁª¥ÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖÂÆûÊó∂Ê®°Êãü},
  author={Ê¢ÅÊµ∑Ááï},
  school={ÁáïÂ±±Â§ßÂ≠¶},
 abstract={‰∫ßÁîüÂºï‰∫∫Ê≥®ÁõÆÁöÑÂä®ÊÄÅÈù¢ÈÉ®Ë°®ÊÉÖÂä®ÁîªÂú®ËÆ°ÁÆóÊú∫ÂõæÂΩ¢Â≠¶‰∏≠ÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊñπÈù¢.ËøëÂπ¥Êù•,ËôöÊãüËßíËâ≤Ë∂äÊù•Ë∂äÂ§öÁöÑÂá∫Áé∞Âú®ËÆ°ÁÆóÊú∫Ê∏∏Êàè,ÂπøÂëä‰ª•ÂèäÁîµÂΩ±Âà∂‰Ωú‰∏≠,‰ΩøÂæóÂÖ∑ÊúâÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖÁöÑËßíËâ≤Âä®ÁîªÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å.Êú¨ÊñáÊèêÂá∫‰∏ÄÁßçÁîüÊàê‰∏âÁª¥ÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖÂÆûÊó∂Âä®ÁîªÁöÑÊñ∞ÊäÄÊúØ,È©±Âä®‰∏âÁª¥Èù¢ÈÉ®ÁΩëÊ†ºÊ®°ÂûãÁîüÊàêÂ∏¶ÊúâÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖÁâπÂæÅËøêÂä®ÁöÑËôöÊãü‰∏âÁª¥ËßíËâ≤Âä®Áîª. È¶ñÂÖà,‰∏∫‰∫ÜÂÆûÊó∂ÊçïËé∑Áî®Êà∑ÁöÑ‰∏çÂêåË°®ÊÉÖÁä∂ÊÄÅÁâπÂæÅ,Âà©Áî®ÂæÆËΩØÁöÑKinect3D‰ΩìÊÑüÊëÑÂΩ±Êú∫ÂØπÁî®Êà∑ÁöÑÈù¢ÈÉ®Ë°®ÊÉÖËøõË°åÂÆûÊó∂ÁöÑË∑üË∏™,ÈÄöËøáÂàÜÊûêÊçïÊçâÂæóÂà∞ÁöÑ‰∫∫ËÑ∏ËøêÂä®Êï∞ÊçÆ,Â∞ÜËøêÂä®Êï∞ÊçÆÂàÜËß£‰∏∫‰∏§‰∏™ÈÉ®ÂàÜ:Â§¥ÈÉ®ÁöÑÂàöÊÄßËøêÂä®ÂíåÈù¢ÈÉ®Ë°®ÊÉÖÁöÑËøêÂä®.Áõ∏ÂØπ‰∫é‰æùËµñÁâπÂÆöÁ°¨‰ª∂ËÆæÂ§áÁöÑ‰∫∫‰ΩìËøêÂä®ÊçïÊçâÁ≥ªÁªüÊù•ËØ¥,KinectÈôç‰Ωé‰∫ÜÁ≥ªÁªüÁöÑÁ°¨‰ª∂ÊàêÊú¨ÂíåË∞ÉËØïÁª¥Êä§Ë¥πÁî®,Âπ∂‰∏îÂØπ‰∫éËá™ÁÑ∂ÁéØÂ¢ÉÁöÑÂ§çÊùÇËÉåÊôØÂÖ∑ÊúâÂæàÂ•ΩÁöÑÈÄÇÂ∫îÊÄß. ÂÖ∂Ê¨°,Âú®Áî®Êà∑Èù¢ÈÉ®Ë°®ÊÉÖÁöÑÊçïËé∑ÂíåÊï∞ÊçÆÂ§ÑÁêÜÁöÑÂü∫Á°Ä‰∏ä,Âà©Áî®ÊãâÊôÆÊãâÊñØÂùêÊ†áÂ±ÄÈÉ®ÁªÜËäÇ‰øùÁïôÁöÑÊÄßË¥®,‰ΩøÁî®ÊãâÊôÆÊãâÊñØÂèòÂΩ¢ÁöÑÊñπÊ≥ïÊääÊçïËé∑ÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÊò†Â∞ÑÂà∞‰∏Ä‰∏™‰∏≠ÊÄßÁöÑ‰∏âÁª¥‰∫∫ËÑ∏Ê®°Âûã‰∏ä,ÂØπËôöÊãüÁöÑ‰∏âÁª¥‰∫∫ËÑ∏Ê®°ÂûãËøõË°åÂßøÊÄÅÈáçÂª∫,‰∫ßÁîüÂÖ∑ÊúâÁùÅÁúºÈó≠ÁúºÂíåÂò¥ÈÉ®ËøêÂä®Á≠â‰∏éÁî®Êà∑Ë°®ÊÉÖÁä∂ÊÄÅ‰∏ÄËá¥ÁöÑËôöÊãü‰∏âÁª¥ËßíËâ≤Âä®Áîª. ÂÜçÊ¨°,‰∏∫‰∫Ü‰∫ßÁîüÂ∏¶ÊúâÁö±Á∫πÁ≠âÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖÁâπÂæÅÁöÑÂÆûÊó∂Ë°®ÊÉÖÂä®Áîª,ÁîüÊàêÂ∏¶ÊúâÊØõÂ≠î,ËÉ°È°ª‰ª•ÂèäÂáπÂá∏‰∏çÂπ≥Á≤óÁ≥ôÊÑüÁöÑÁöÆËÇ§Á∫πÁêÜ,Âà©Áî®GPUËøõË°åÂÖâÁÖßÂíåÊ≥ïÁ∫øË¥¥ÂõæÊ∏≤Êüì,ÂÜçÊ†πÊçÆKinectÊçïËé∑ÁöÑÂä®‰ΩúÂçïÂÖÉËÆ°ÁÆóÂä®ÊÄÅÁ∫πÁêÜÊò†Â∞ÑÂíåÁö±Á∫π‰∫ßÁîüÁöÑÊùÉÈáç,Âπ∂ÈÄöËøáÂºïÂÖ•Áö±Á∫πÂáΩÊï∞ÂÆûÊó∂Ê®°ÊãüÁö±Á∫πËøêÂä®Áä∂ÊÄÅ,‰∫ßÁîüÂä®‰ΩúÈ©±Âä®ÁöÑÈù¢ÈÉ®Ë°®ÊÉÖÂÆûÊó∂Âä®Áîª. ÊúÄÂêé,Âà©Áî®‰∏ì‰∏öÂõæÂΩ¢Á®ãÂ∫èÊé•Âè£OpenGLÂíåÈ´òÁ∫ßÁùÄËâ≤ËØ≠Ë®ÄGLSLËÆæËÆ°Âπ∂ÂÆûÁé∞‰∫ÜÂÆûÊó∂ÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖ‰ªøÁúüÁ≥ªÁªü.ÂÆûÈ™åË°®Êòé,Âà©Áî®Êú¨ÊñáÁöÑÊñπÊ≥ïÂèØ‰ª•‰∫ßÁîüÂä®‰ΩúÈ©±Âä®ÁöÑÈÄºÁúüÁªÜÂæÆÈù¢ÈÉ®Ë°®ÊÉÖÂÆûÊó∂Âä®Áîª,ÈÄÇÁî®‰∫éÊï∞Â≠óÂ®±‰πê,ËßÜÈ¢ë‰ºöËÆÆÁ≠âÈ¢ÜÂüü.},
}

@phdthesis{p13,
  title={Êï∞ÊçÆÈ©±Âä®ÁöÑ‰∫∫‰ΩìÈ™®È™ºËøêÂä®ÂêàÊàêÁ†îÁ©∂},
  author={Ê®äÁíê},
  school={ÈïøÂÆâÂ§ßÂ≠¶},
 abstract={‰∫∫‰ΩìËøêÂä®ÂêàÊàêÊäÄÊúØÊòØËôöÊãüÁé∞ÂÆûÈ¢ÜÂüü‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçÁÇπÁ†îÁ©∂ÂÜÖÂÆπ,Âú®Â®±‰πê,ÊïôËÇ≤ÂíåÂÜõ‰∫ãÁ≠âÈ¢ÜÂüüË¢´ÂπøÊ≥õÂ∫îÁî®.Ê®°ÂûãÈ©±Âä®ÂíåÊï∞ÊçÆÈ©±Âä®ÊòØÁõÆÂâçËøêÂä®ÂêàÊàêÁöÑ‰∏§‰∏™‰∏ªË¶ÅÈÄîÂæÑ,Êú¨ÊñáÈááÁî®ÁöÑÂü∫‰∫éÊï∞ÊçÆÈ©±Âä®ÁöÑ‰∫∫‰ΩìËøêÂä®ÂêàÊàêÊõ¥Êòì‰∫éÊûÑÂª∫ÈÄºÁúüÂ∫¶È´ò,Ëá™ÁÑ∂ÊÄßËâØÂ•ΩÁöÑ‰∫∫‰ΩìÂä®Áîª.ÂΩìÂâç,Áî±‰∫éËøêÂä®ÊçïÊçâÊäÄÊúØÂà∂‰ΩúÊàêÊú¨ËæÉÂ§ß,Âä†‰πã‰∫∫‰ΩìËøêÂä®ÁöÑÈ´òÂ∫¶Â§çÊùÇÊÄß,ÁªôËøêÂä®Êï∞ÊçÆÈáçÁî®Â∏¶Êù•ÂæàÂ§ßÈòªÁ¢ç.ÈíàÂØπËøôÁßçÊÉÖÂÜµ,ËÆ∫ÊñáÂõ¥Áªï‰∫∫‰ΩìËøêÂä®ÊçïÊçâÊï∞ÊçÆ,Â±ïÂºÄÂØπ‰∫∫‰ΩìÈ™®È™ºËøêÂä®ÂêàÊàêÁöÑÂÖ≥ÈîÆÊäÄÊúØÂíåÊñπÊ≥ïËøõË°åÁ†îÁ©∂.ÂÖ∑‰ΩìÁ†îÁ©∂ÂÜÖÂÆπÂ¶Ç‰∏ã:1,Âçï‰∏™ËøêÂä®Â∫èÂàóÁöÑÁºñËæë.Áî±‰∫éÊØè‰∏™ËøêÂä®Â∫èÂàóÁöÑÊúùÂêë,‰ΩçÁßª‰ø°ÊÅØ‰∏ç‰∏ÄÊ†∑,Ë¶Å‰øùËØÅ‰∏§‰∏™Êàñ‰∏§‰∏™‰ª•‰∏äËøêÂä®Â∫èÂàóÁöÑÂêàÊàê,È¶ñÂÖàË¶Å‰ΩøÂæÖÂêàÊàêÁöÑËøêÂä®Â∫èÂàóÊúùÂêëÂíåË°åÂæÑÊñπÂêëÂ§ß‰Ωì‰∏ÄËá¥,ËøôÊòØ‰∫∫‰ΩìÈ™®È™ºËøêÂä®ÂêàÊàêÁöÑÂü∫Á°Ä.Êú¨ÊñáÈááÁî®Áü©ÈòµÊóãËΩ¨,Âπ≥ÁßªÂíåÁº©ÊîæÊìç‰ΩúÂØπÂçï‰∏™ËøêÂä®Â∫èÂàóÊï∞ÊçÆËøõË°åËÆ°ÁÆó,‰ΩøÂæóÂèØ‰ª•‰ªªÊÑèÊîπÂèòÊüê‰∏™ËøêÂä®Â∫èÂàóÂú®ËôöÊãüÂú∫ÊôØ‰∏≠ÁöÑËøêÂä®‰ø°ÊÅØ.2,‰∫∫‰ΩìËøêÂä®Áõ∏‰ººÊÄßËÆ°ÁÆó.Âè™ÊúâÁõ∏‰ººÂπ∂Â≠òÂú®ËæÉ‰∏∫Ëá™ÁÑ∂ÁöÑËøêÂä®ËøáÊ∏°ÂèØËÉΩÁöÑËøêÂä®Â∫èÂàó‰πãÈó¥ÊâçËÉΩËøõË°åÊúâÊïàÁöÑËøêÂä®ÂêàÊàê.Êú¨ÊñáÈááÁî®Ê¨ßÂºèË∑ùÁ¶ªËÆ°ÁÆóÁ≠âÈïøËøêÂä®Â∫èÂàóÁöÑÁõ∏‰ººÊÄß;‰ΩøÁî®Âä®ÊÄÅÊó∂Èó¥ÂèòÂΩ¢ÁÆóÊ≥ï(DTW)ÂÆåÊàê‰∏çÁ≠âÈïøËøêÂä®Â∫èÂàóÈó¥ÁöÑÁõ∏‰ººÊÄßËÆ°ÁÆó.3,Â∏∏Áî®ËøêÂä®ËøáÊ∏°ÊñπÊ≥ïÂÆûÈ™åÊØîËæÉ.ÂØπÂü∫‰∫éÁ©∫Èó¥‰ΩçÁΩÆÊèíÂÄºÂêàÊàê,Âü∫‰∫é‰∫∫‰ΩìËøêÂä®ÂßøÂäøÁöÑÂõõÂÖÉÊï∞ÊèíÂÄºÂíåÂü∫‰∫éÈÄÜÂêëËøêÂä®Â≠¶ÁöÑÊèíÂÄºÁÆóÊ≥ïËøõË°åÂÆûÈ™å,ÂàÜÊûêÊØîËæÉ‰∏â‰∏™ÁÆóÊ≥ïÁöÑÂÆûÈ™åÁªìÊûú,ÂæóÂá∫ÂÆÉ‰ª¨Â≠òÂú®ÊâãÂ∑•‰∫§‰∫íÈáèÂ§ßÂíåÊ†∑Êú¨‰æùËµñÊÄßÂ§ßÁöÑÁªìËÆ∫.4,Âü∫‰∫éÂ§öÁª¥ÊãâÊôÆÊãâÊñØÂùêÊ†áÁöÑËøêÂä®ËøáÊ∏°ÁºñËæë.‰∏âÁª¥ÊãâÊôÆÊãâÊñØÂùêÊ†áÂèØ‰ª•ÊúâÊïà‰øùÊåÅ‰∏âÁª¥Á©∫Èó¥ÂÜÖÂá†‰ΩïÊï∞ÊçÆÁöÑÂ±ÄÈÉ®ÁâπÂæÅ,Áî±‰∫é‰∫∫‰ΩìÊçïÊçâËøêÂä®Êï∞ÊçÆÁöÑÁª¥Â∫¶È´ò,‰∏∫‰∫Ü‰øùÊåÅËøêÂä®Â∫èÂàóÊúâÊïàÊéßÂà∂ÂÖ≥ËäÇÁÇπÂú®Êó∂Èó¥‰∏äÁöÑÊó∂Â∫èÁâπÊÄß,Êú¨ÊñáÈááÁî®Â§öÁª¥ÁöÑÊãâÊôÆÊãâÊñØÂùêÊ†á.Â∞Ü‰∏§ÊÆµÂæÖÂêàÊàêÁöÑ‰∫∫‰ΩìËøêÂä®Êï∞ÊçÆÊò†Â∞Ñ‰∏∫Â§öÁª¥ÂêëÈáèÁ©∫Èó¥‰∏≠Áî®È°∂ÁÇπÈõÜÂêàË°®Á§∫ÁöÑÊõ≤Á∫øÊÆµ,ÊØè‰∏™ÁÇπ‰ª£Ë°®ËøêÂä®Êï∞ÊçÆÊüê‰∏ÄÂ∏ßËã•Âπ≤‰∏™ÈÄöÈÅìÁöÑÊï∞ÊçÆ‰ø°ÊÅØ,Êï∞ÊçÆÈó¥ÁöÑÊó∂Â∫èÂÖ≥Á≥ªÂàôÊòØÁî®ÁÇπ‰πãÈó¥ÁöÑÈÇªÂüüÂÖ≥Á≥ªÊù•Ë°®Á§∫ÁöÑ,‰ΩøÁî®Â§öÁª¥ÊãâÊôÆÊãâÊñØÂùêÊ†á‰øùÊåÅÊ≥ïÂØπÊò†Â∞ÑÁöÑ‰∏§ÊÆµÊõ≤Á∫øËøõË°åÁºñËæëÂíåÊãºÊé•,ÁîüÊàêËøêÂä®ËøáÊ∏°Â∫èÂàóÁâáÊÆµ.ÂÆûÈ™åË°®Êòé,Â§öÁª¥ÊãâÊôÆÊãâÊñØÂùêÊ†áÁöÑËøêÂä®ÂêàÊàêÊñπÊ≥ï‰∫∫Êú∫‰∫§‰∫íÈáèÂ∞ë,Ê†∑Êú¨‰æùËµñÁ®ãÂ∫¶ËæÉ‰Ωé,ÂèØ‰ª•ÊèêÈ´òÂ∑≤ÊúâÊï∞ÊçÆÁöÑÈáçÁî®ÊÄß.},
}

@inproceedings{p12,
  title={Optimizing Network Structure for 3D Human Pose Estimation},
  author={ Hai, C.  and  Wang, C.  and  Ma, X.  and  Wang, Y. },
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2019},
 abstract={A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets.},
}

@book{p11,
  title={Propagating LSTM: 3D Pose Estimation Based on Joint Interdependency},
  author={ Lee, K.  and  Lee, I.  and  Lee, S. },
  publisher={Computer Vision ‚Äì ECCV 2018},
  year={2018},
 abstract={We present a novel 3D pose estimation method based on joint interdependency (JI) for acquiring 3D joints from the human pose of an RGB image. The JI incorporates the body part based structural...},
}

@phdthesis{p10,
  title={Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁº∫Èô∑Ê£ÄÊµãÁÆóÊ≥ïÁ†îÁ©∂},
  author={Â∫ûÂçö},
  school={Ë•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶},
  year={2020},
 abstract={ÊàëÂõΩÊòØÂà∂ÈÄ†‰∏öÂ§ßÂõΩ,Âà∂ÈÄ†Áîü‰∫ßÂéÇÂïÜ‰ºóÂ§ö„ÄÇÁé∞Âú®Â§ßÈÉ®ÂàÜÁöÑÁîü‰∫ß‰ºÅ‰∏ö‰æùÁÑ∂ÈááÁî®‰º†ÁªüÁöÑÊú∫Âô®ËßÜËßâÊñπÊ≥ï(Âç≥ÈÄöËøáÂØπÂõæÂÉèÁöÑÈòàÂÄºÂàÜÂâ≤„ÄÅÊª§Ê≥¢„ÄÅÈ¢úËâ≤ÁªüËÆ°Á≠âÊñπÊ≥ï)ÁîöËá≥Áî®‰∫∫Â∑•ÂéªÊ£ÄÊµãÁº∫Èô∑„ÄÇ‰ΩÜÊòØÈöèÁùÄÁîü‰∫ßÊ∞¥Âπ≥‰∏çÊñ≠Âú∞ÊèêÈ´ò,‰º†ÁªüÁöÑÊú∫Âô®ËßÜËßâÂ§ÑÁêÜÊñπÊ≥ïÂú®Á≤æÂ∫¶‰∏äÊÖ¢ÊÖ¢Êó†Ê≥ïÊª°Ë∂≥Âà∂ÈÄ†‰∏öÁöÑÈúÄÊ±Ç,ÂêåÊó∂‰πüÈöæ‰ª•Ëß£ÂÜ≥Áº∫Èô∑ÁöÑÁõÆÊ†áÊ£ÄÊµãÈóÆÈ¢ò„ÄÇ‰∏éÊ≠§ÂêåÊó∂,Ê∑±Â∫¶Â≠¶‰π†Ëì¨ÂãÉÂèëÂ±ï,Âπ∂‰∏îÊ∑±Â∫¶Â≠¶‰π†Âú®ÂæàÂ§öÈ¢ÜÂüüÂèñÂæó‰∫Ü‰ª§‰∫∫ÊÉäÂñúÁöÑÊàêÊûú„ÄÇÊâÄ‰ª•ÂæàËá™ÁÑ∂ÁöÑ‰∫∫‰ª¨ÂºÄÂßãÂ∞ÜÁº∫Èô∑Ê£ÄÊµã‰∏éÊ∑±Â∫¶Â≠¶‰π†ËÅîÁ≥ªÂú®‰∏ÄËµ∑„ÄÇÂú®Áº∫Èô∑Ê£ÄÊµãÈóÆÈ¢ò‰∏≠,ÊúÄ‰∏∫Âü∫Êú¨ÁöÑÈóÆÈ¢òÊòØÁº∫Èô∑ÂàÜÁ±ªÈóÆÈ¢ò„ÄÇÊú¨ÊñáÈÄöËøáÁ†îÁ©∂‰∏âÁßçÁªèÂÖ∏ÁöÑÁ•ûÁªèÁΩëÁªúÊ®°Âûã:VGGNet„ÄÅResNet‰ª•ÂèäGoogLeNet,ÂàÜÊûêÂπ∂Â≠¶‰π†‰ªñ‰ª¨ÁöÑ‰ºòÁÇπÂíåÊÄùË∑Ø,ÊèêÂá∫‰∫ÜÊîπËøõÂêéÁöÑ‰∏âÁßçÁ•ûÁªèÁΩëÁªú:VGGNet-ch„ÄÅResNet-ch‰ª•ÂèäGoogLeNet-ch„ÄÇËøô‰∏âÁßçÁΩëÁªúÈÄöËøáË∞ÉÊï¥ÂÖ∂ËæìÂÖ•ËæìÂá∫Â∞∫ÂØ∏,ÊîπÂèòÂç∑ÁßØÂ±ÇÁöÑÊï∞ÈáèÂèäÂç∑ÁßØÊ†∏Â∞∫ÂØ∏,Â¢ûÂä†ÊàñÂà†Èô§ÂÖ®ËøûÊé•Â±ÇÂèäÂÖ∂‰∏≠Á•ûÁªèÂÖÉÁöÑÊï∞Èáè,ÊûÑÈÄ†Âá∫Êõ¥ÈÄÇÂêàÊèêÂèñÁâπÂæÅÁöÑÁΩëÁªú„ÄÇÂ∞Ü‰∏âÁßçÁΩëÁªúÂ∫îÁî®Âú®Á´πÁ≠∑Áº∫Èô∑ÂàÜÁ±ªÈóÆÈ¢ò‰∏ä,ÂÖ∂ÂáÜÁ°ÆÁéáÁõ∏ÂØπÂéüÂßãÁΩëÁªúÊúâÊòéÊòæÊèêÈ´ò„ÄÇÂêåÊó∂,‰∏∫‰∫ÜÂáèÂ∞ëÂú®Á•ûÁªèÁΩëÁªúÊûÑÂª∫Êó∂‰∫∫Â∑•Ë∞ÉÂèÇÁöÑÈ¢ëÁéá,‰ªéËÄåÈôç‰ΩéËß£ÂÜ≥Áº∫Èô∑ÂàÜÁ±ªÈóÆÈ¢òÁöÑÈöæÂ∫¶,Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÁ•ûÁªèÁΩëÁªúÁªìÊûÑÊêúÁ¥¢ÁÆóÊ≥ï:Á•ûÁªèÁΩëÁªúÊ∑±Â∫¶Ëá™Â¢ûÈïøÁÆóÊ≥ï‰ª•ÂèäÂü∫‰∫éÁÆÄÂçïÁà¨Â±±ÁÆóÊ≥ïÁöÑÁ•ûÁªèÁΩëÁªúÁªìÊûÑÊêúÁ¥¢ÁÆóÊ≥ï„ÄÇÈÄöËøá‰ΩøÁî®Ëøô‰∏§ÁßçÁ•ûÁªèÁΩëÁªúÁªìÊûÑÊêúÁ¥¢ÁÆóÊ≥ï,Á•ûÁªèÁΩëÁªúËÉΩÂ§üÂú®ËÆ≠ÁªÉ‰∏≠‰∏çÊñ≠Âú∞Â¢ûÂä†Âç∑ÁßØÂ±ÇÊàñÈôçÈááÊ†∑Â±Ç,‰ªéËÄå‰ΩøÂæóÁ•ûÁªèÁΩëÁªúÁöÑÊ∑±Â∫¶Êõ¥Ê∑±„ÄÅÂÆΩÂ∫¶Êõ¥Âπø,ÊúÄÁªàÂèØ‰ª•ËÆ≠ÁªÉÂæóÂà∞‰∏Ä‰∏™ÈÄÇÂêàÂΩìÂâçÁº∫Èô∑Êï∞ÊçÆÈõÜÁöÑÁ•ûÁªèÁΩëÁªú„ÄÇÂú®ÂΩìÂâçÊï∞ÊçÆÈõÜ‰∏ä,ËØ•Á•ûÁªèÁΩëÁªúËÉΩÂ§üÂÅöÂà∞‰∏éÊâãÂ∑•Ë∞ÉÂèÇÊâÄÂæóÁ•ûÁªèÁΩëÁªúÁõ∏ËøëÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá„ÄÇÁº∫Èô∑Ê£ÄÊµã‰∏≠‰∏çÊ≠¢ÊúâÁº∫Èô∑ÂàÜÁ±ªÈóÆÈ¢ò,ÂêåÊó∂Ëøò‰ºöÊúâÁº∫Èô∑ÁõÆÊ†áÊ£ÄÊµãÈóÆÈ¢ò„ÄÇÊú¨ÊñáÈíàÂØπÁ´πÁ≠∑ÁöÑÁº∫Èô∑ÁõÆÊ†áÊ£ÄÊµãÈóÆÈ¢ò,È¶ñÂÖàÁ†îÁ©∂‰∫Ü‰∏§ÁßçÁªèÂÖ∏ÁöÑÁõÆÊ†áÊ£ÄÊµãÁÆóÊ≥ï:Faster-RCNN‰ª•ÂèäSSD,Âπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁº∫Èô∑ÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãSSD-ch„ÄÇSSD-chÂ∞ÜSSD‰∏éÊú¨ÊñáÊâÄÊèêÂá∫ÁöÑVGGNet-chÁõ∏ÁªìÂêà,ÂáèÂ∞ë‰∫ÜÁ•ûÁªèÁΩëÁªú‰∏≠ÂèÇÊï∞ÁöÑÊï∞Èáè,Âπ∂‰ΩøÂæóÂÖ∂‰∏≠ÁöÑÂàùÂßãÁâπÂæÅÊèêÂèñÁΩëÁªúÊõ¥ÈÄÇÂêàÊèêÂèñÁ´πÁ≠∑ÁöÑÁâπÂæÅ„ÄÇÂ∫îÁî®Âú®Á´πÁ≠∑ÁöÑÁõÆÊ†áÊ£ÄÊµãÊó∂,ËÆ≠ÁªÉÂæóÂà∞ÁöÑSSD-chËÉΩÂ§üÂú®‰øùÊåÅËæÉÈ´òmAPÁöÑÂêåÊó∂,ÈùûÂ∏∏ÊòéÊòæÂú∞ÊèêÈ´òÁΩëÁªúÁöÑËøêË°åÈÄüÂ∫¶„ÄÇÊ≠§Â§ñÊú¨ÊñáËøòÈíàÂØπÂ∏¶Á∫πÁêÜÁöÑÁ∫∫ÁªáÂìÅÁº∫Èô∑‰ΩçÁΩÆÊ†áÊ≥®ÈóÆÈ¢ò,ÊèêÂá∫‰∫ÜÊîπËøõÁöÑÂ†ÜÂè†ÈôçÂô™Âç∑ÁßØËá™ÁºñÁ†ÅÂô®ÁÆóÊ≥ï‰ª•ÂèäËûçÂêàËá™ÁºñÁ†ÅÂô®‰∏éÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÁöÑÁº∫Èô∑‰ΩçÁΩÆÊ†áÊ≥®ÁÆóÊ≥ï„ÄÇËøô‰∏§ÁßçÁÆóÊ≥ïËÉΩÂ§üÈÄöËøáÊûÑÂª∫‰∏ÄÁªÑÊ∑±Â∫¶ËæÉÊµÖ„ÄÅÂèÇÊï∞ËæÉÂ∞ëÁöÑÁ•ûÁªèÁΩëÁªú,ÂØπÁº∫Èô∑ÂõæÂÉèËøõË°åËøòÂéü„ÄÇÁÑ∂ÂêéÈÄöËøáÂ∞ÜËøòÂéüÂêéÁöÑÂõæÂÉè‰∏éÂéüÂõæÂÉèËøõË°å‰ΩúÂ∑Æ„ÄÅÈòàÂÄºÂ§ÑÁêÜÁ≠âÊìç‰Ωú,ËÉΩÂ§üËæÉ‰∏∫ÂáÜÁ°ÆÂú∞ÂæóÂà∞ÂæÖÊ£ÄÊµãÂõæÂÉè‰∏≠Áº∫Èô∑ÁöÑÂÉèÁ¥†Á∫ßÁ≤æÂ∫¶ÁöÑÂÖ∑‰Ωì‰ΩçÁΩÆ„ÄÇ},
}

@phdthesis{p9,
  title={Èù¢ÂêëÂÆ§Â§ñÂõæÂÉèÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁÆóÊ≥ïÁ†îÁ©∂},
  author={Á®ãÊ≠£Ê∂õ},
  school={ÂìàÂ∞îÊª®Â∑•‰∏öÂ§ßÂ≠¶},
  year={2019},
 abstract={Âü∫‰∫éÂõæÂÉèÁöÑ‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÊòØÊåá‰ªéÂõæÂÉè‰∏≠Ê£ÄÊµã‰∫∫‰ΩìÂêÑÈÉ®ÂàÜÁöÑ‰ΩçÁΩÆÂπ∂ËÆ°ÁÆóÂÖ∂ÊñπÂêëÂíåÂ∞∫Â∫¶‰ø°ÊÅØÁöÑËøáÁ®ã,ËÆ°ÁÆóÁöÑÁªìÊûúÂàÜ‰∏∫‰∫åÁª¥Âíå‰∏âÁª¥‰∏§ÁßçÊÉÖÂÜµ,Êú¨Êñá‰∏ªË¶ÅÁ†îÁ©∂ÂçïÂº†ÂÆ§Â§ñRGBÂõæÂÉèÁöÑ‰∏âÁª¥‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÈóÆÈ¢ò,Âç≥ËÆ°ÁÆóÂú®ÂçïÂº†RGBÂõæÂÉè‰∏≠‰∫∫‰ΩìÂêÑÂÖ≥ËäÇÁÇπÁöÑ‰∏âÁª¥ÂùêÊ†á‰ø°ÊÅØ,ÈáçÂª∫‰∫∫‰ΩìÁöÑ‰∏âÁª¥ÂßøÊÄÅÊ®°Âûã.‰ΩÜÊòØÁî±‰∫éÊï∞ÊçÆÈõÜÊ†áÊ≥®Âõ∞Èöæ,Âú®‰∏âÁª¥‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°È¢ÜÂüü,Áº∫Â∞ëÂÆ§Â§ñÈùûÂèóÈôêÂú∫ÊôØ‰∏ãÈááÈõÜÁöÑÂÖ∑Êúâ‰∏âÁª¥‰ø°ÊÅØÊ†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜ.ÈíàÂØπÊï∞ÊçÆÈõÜÁº∫Â§±ÈóÆÈ¢ò,Êú¨ËØæÈ¢òÂÄüÈâ¥‰∏ÄÁßçÂü∫‰∫éÂº±ÁõëÁù£ËøÅÁßªÂ≠¶‰π†ËøõË°åÂÆ§Â§ñÂõæÂÉèÁöÑ‰∏âÁª¥‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°ÁöÑÊñπÊ≥ï,‰ª•ÂÆ§Â§ñÂ§çÊùÇÂú∫ÊôØ‰∏ãÂÖ∑Êúâ‰∫åÁª¥‰ø°ÊÅØÊ†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜÂíåÂÖ∑Êúâ‰∏âÁª¥‰ø°ÊÅØÁöÑÂÆ§ÂÜÖÊï∞ÊçÆÈõÜÂêåÊó∂‰Ωú‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆ,ÂæóÂà∞‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑ‰∏âÁª¥‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Ê®°Âûã,ÂÆûÁé∞ÂØπÂÆ§Â§ñÂ§çÊùÇÂú∫ÊôØ‰∏ãÁöÑ‰∏âÁª¥‰∫∫‰ΩìÂßøÊÄÅÊ®°ÂûãÁöÑÈáçÂª∫.Âú®Ê®°ÂûãÊû∂ÊûÑÊñπÈù¢,Âº±ÁõëÁù£ËøÅÁßªÂ≠¶‰π†ÈááÁî®‰∏§Èò∂ÊÆµÊñπÊ≥ï,2DÂßøÊÄÅ‰º∞ËÆ°Â≠êÊ®°ÂûãÈááÁî®Â†ÜÂè†Ê≤ôÊºèÊ®°ÂûãÂÆåÊàê2DÂßøÊÄÅ‰º∞ËÆ°Âíå‰∏ªË¶ÅÁöÑÁâπÂæÅÊèêÂèñÂ∑•‰Ωú,ËæìÂá∫ÁöÑ2DÂßøÊÄÅ‰ø°ÊÅØÂíå‰∏≠Èó¥ÁâπÂæÅÂ∞Ü‰ºöÂÖ±‰∫´ÁªôÊ∑±Â∫¶ÂõûÂΩíÊ®°Âûã,ËøõË°åÊ∑±Â∫¶‰ø°ÊÅØÁöÑÂõûÂΩí.Êú¨ÊñáÁ†îÁ©∂ÂàÜÊûê‰∫ÜÂº±ÁõëÁù£ËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ïÁöÑÂü∫Êú¨ÂéüÁêÜÂíåÊµÅÁ®ã,Âπ∂Â§öÊ¨°‰ΩøÁî®Áõ∏ÂêåÂèÇÊï∞ÂØπÊ®°ÂûãËøõË°åËÆ≠ÁªÉÂíåÊµãËØï,ÂèëÁé∞Ê®°ÂûãÂÖ∑ÊúâÊî∂ÊïõÊõ≤Á∫øÊ≥¢Âä®Â§ß,ËÆ≠ÁªÉÂèäÊµãËØïÁªìÊûúÂ∑ÆÂºÇÂ§ßÁöÑÁâπÁÇπ.Âú®ËøÅÁßªÂ≠¶‰π†È¢ÜÂüü,ÊèêÂèñÁöÑÁâπÂæÅÊòØÂê¶"ÊôÆÈÄÇ"ÂØπ‰∫éËøÅÁßªÂ≠¶‰π†Ê®°ÂûãÁöÑÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄßÊúâÂæàÂ§ßÂΩ±Âìç.ÊâÄ‰ª•‰∏∫ÊèêÈ´òÊ®°ÂûãÁ≤æÂ∫¶ÂíåÊ®°ÂûãÈ≤ÅÊ£íÊÄß,Êú¨ÊñáÂÄüÈâ¥Ëá™Ê≠•Â≠¶‰π†ÊÄùÊÉ≥ÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Ê≠•ÂºèËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ï,ËÄÉËôëËÆ≠ÁªÉÊ†∑Êú¨ËæìÂÖ•È°∫Â∫èÂØπÊúÄÁªàÊ®°ÂûãÊÄßËÉΩÂèäÈ≤ÅÊ£íÊÄßÁöÑÂΩ±Âìç,"Áî±ÊòìÂà∞Èöæ,ÂÖàÂø´ÂêéÊÖ¢"ÁöÑÂØπÊ†∑Êú¨Á©∫Èó¥ËøõË°åÂ≠¶‰π†,ÂÖãÊúç‰∫ÜÊ®°ÂûãÈ≤ÅÊ£íÊÄßÂ∑ÆÁöÑÈóÆÈ¢ò,ÂêåÊó∂‰∏ÄÂÆöÁ®ãÂ∫¶ÊèêÂçá‰∫ÜÊ®°ÂûãÁ≤æÂ∫¶.Ê∑±Â∫¶ÂõûÂΩíÂ≠êÊ®°ÂûãÈááÁî®Â†ÜÂè†Ê≤ôÊºèÊ®°ÂûãÊèêÂèñÁöÑ‰∏≠Èó¥ÁâπÂæÅÂØπÊ∑±Â∫¶‰ø°ÊÅØËøõË°åÂõûÂΩí,‰ΩÜÂ†ÜÂè†Ê≤ôÊºèÊ®°Âûã‰∏ªË¶ÅËß£ÂÜ≥‰∫åÁª¥ÂßøÊÄÅ‰º∞ËÆ°ÈóÆÈ¢ò,‰∏≠Èó¥ÁâπÂæÅÂØπÊ∑±Â∫¶‰ø°ÊÅØË°®ËææËÉΩÂäõËæÉÂº±.Êú¨ÊñáÂÄüÈâ¥SENetÊ®°ÂùóÁªìÊûÑ,ÊèêÂá∫ÂèçÈ¶àÂºèÁâπÂæÅÊùÉÈáçÈáçÂàÜÈÖçÁ≠ñÁï•,‰∏ç‰ªÖÂ¢ûÂº∫ÁâπÂæÅÊ∑±Â∫¶‰ø°ÊÅØË°®ËææËÉΩÂäõ,ËÄå‰∏îÂ¢ûÂº∫ÂâçÂêéÊ†∑Êú¨ÁâπÂæÅÂÖ≥ËÅîÊÄß,ÊèêÂçáÊ®°ÂûãÈ≤ÅÊ£íÊÄß.ÂêåÊó∂Âú®‰∏äÈááÊ†∑Èò∂ÊÆµÈááÁî®ÂèåÁ∫øÊÄßÊèíÂÄºÊñπÊ≥ï,Â¢ûÂº∫Ê®°ÂûãÁâπÂæÅË°®ËææËÉΩÂäõ,‰∏∫Ê∑±Â∫¶ÂõûÂΩíÁéØËäÇÊèê‰æõ‰∫ÜËâØÂ•ΩÁöÑÂü∫Á°Ä.Êú¨Êñá‰∏ªË¶Å‰ΩøÁî®MPII,Human3.6MÂÖ¨ÂºÄÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉ,‰ΩøÁî®Human3.6M,MPI-INF-3DHPÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÁöÑÊµãËØïÈõÜËøõË°åÊ®°ÂûãÊµãËØï.ÂÆûÈ™åÁªìÊûúËØÅÂÆû‰∫ÜÊú¨ÊñáÊîπËøõÂêéÁöÑËá™Ê≠•ÂºèËøÅÁßªÂ≠¶‰π†Ê®°ÂûãËÉΩÂ§üÊúâÊïàÊèêÂçá‰∫∫‰ΩìÂßøÊÄÅ‰º∞ËÆ°Ê®°ÂûãÁöÑÈáçÂª∫Á≤æÂ∫¶.Âú®Human3.6MÂÖ¨ÂºÄÊï∞ÊçÆÈõÜ‰∏äËøõË°åÊµãËØïÊó∂MPJPE‰∏∫60.69mm,PCKhËææÂà∞92.3%,Áõ∏ÂØπÂü∫ÂáÜÊ®°ÂûãÂàÜÂà´ÊèêÂçá‰∫Ü4.21 mmÂíå0.7%.Âú®MPI-INF-3DHPÂÖ¨ÂºÄÊï∞ÊçÆÈõÜ‰∏äËøõË°åÊµãËØïÊó∂MPJPE‰∏∫41.35mm,PCKhËææÂà∞90.84%,Áõ∏ÂØπÂü∫ÂáÜÊ®°ÂûãÂàÜÂà´ÊèêÂçá‰∫Ü17.35 mmÂíå6.22%.},
}

@article{p8,
  title={Exploiting temporal information for 3D pose estimation},
  author={ Hossain, Mir Rayat Imtiaz  and  Little, James J },
  journal={Springer, Cham},
  year={2017},
 abstract={In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately 12.2\% 12.2\%  and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.},
}

@article{p7,
  title={2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning},
  author={ Luvizon, D. C.  and  Picard, D.  and  Tabia, H. },
  journal={IEEE},
  year={2018},
 abstract={Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.},
}

@inproceedings{p6,
  title={3D Human Pose Estimation in the Wild by Adversarial Learning},
  author={ Yang, W.  and  Ouyang, W.  and  Wang, X.  and  Ren, J.  and  Li, H.  and  Wang, X. },
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
 abstract={Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.},
}

@inproceedings{p5,
  title={Ordinal Depth Supervision for 3D Human Pose Estimation},
  author={ Pavlakos, G.  and  Zhou, X.  and  Daniilidis, K. },
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
 abstract={Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural im- ages. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these or- dinal relations in different settings, always achieving com- petitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the po- tential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This exten- sion allows us to present quantitative and qualitative evalu- ation in non-studio conditions. Simultaneously, these ordi- nal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.},
}

@article{p4,
  title={Learning Pose Grammar to Encode Human Body Configuration for 3D Pose Estimation},
  author={ Fang, H.  and  Xu, Y.  and  Wang, W.  and  Liu, X.  and  Zhu, S. C. },
  year={2017},
 abstract={In this paper, we propose a pose grammar to tackle the problem of 3D human pose estimation. Our model directly takes 2D pose as input and learns a generalized 2D-3D mapping function. The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNN) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a pose sample simulator to augment training samples in virtual camera views, which further improves our model generalizability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.},
}

@article{p3,
  title={Compositional Human Pose Regression},
  author={ Sun, X.  and  Shang, J.  and  Liang, S.  and  Wei, Y. },
  journal={Computer Vision and Image Understanding},
  year={2017},
 abstract={Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M [20] and is competitive with state-of-the-art results on MPII [3].},
}

@inproceedings{p2,
  title={Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation},
  author={ Tekin, B.  and P M√°rquez-Neila and  Salzmann, M.  and  Fua, P. },
  booktitle={IEEE Computer Society},
  year={2016},
 abstract={Most recent approaches to monocular 3D human pose estimation rely on Deep Learning. They typically involve regressing from an image to either 3D joint coordinates directly or 2D joint locations from which 3D coordinates are inferred. Both approaches have their strengths and weaknesses and we therefore propose a novel architecture designed to deliver the best of both worlds by performing both simultaneously and fusing the information along the way. At the heart of our framework is a trainable fusion scheme that learns how to fuse the information optimally instead of being hand-designed. This yields significant improvements upon the state-of-the-art on standard 3D human pose estimation benchmarks.},
}

@inproceedings{p1,
  title={Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose},
  author={ Pavlakos, G.  and  Zhou, X.  and  Derpanis, K. G.  and  Daniilidis, K. },
  booktitle={IEEE Conference on Computer Vision, Pattern Recognition},
  year={2017},
 abstract={This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.},
}




